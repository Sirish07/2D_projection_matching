{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c3213aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import startup\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from models import models\n",
    "import matplotlib\n",
    "if os.environ.get('DISPLAY','') == '':\n",
    "    print('no display found. Using non-interactive Agg backend')\n",
    "    matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from util.system import setup_environment\n",
    "from util.train import get_trainable_variables, get_learning_rate_origin, get_learning_rate, get_path\n",
    "from util.losses import regularization_loss\n",
    "from util.fs import mkdir_if_missing\n",
    "from util.data import tf_record_compression\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy.io import loadmat\n",
    "tfsum = tf.contrib.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df8cd393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_records(cfg, serialized):\n",
    "    num_views = cfg.num_views\n",
    "    image_size = cfg.image_size\n",
    "\n",
    "    # A dictionary from TF-Example keys to tf.FixedLenFeature instance.\n",
    "    features = {\n",
    "        'name': tf.FixedLenFeature([1], tf.string),\n",
    "        'image': tf.FixedLenFeature([num_views, image_size, image_size, 3], tf.float32),\n",
    "        'mask': tf.FixedLenFeature([num_views, image_size, image_size, 1], tf.float32),\n",
    "        'inpoints':tf.FixedLenFeature([num_views, cfg.gt_point_n, 2], tf.float32),\n",
    "    }\n",
    "\n",
    "    if cfg.saved_camera:\n",
    "        features.update(\n",
    "            {'extrinsic': tf.FixedLenFeature([num_views, 4, 4], tf.float32),\n",
    "             'cam_pos': tf.FixedLenFeature([num_views, 3], tf.float32)})\n",
    "    if cfg.saved_depth:\n",
    "        features.update(\n",
    "            {'depth': tf.FixedLenFeature([num_views, image_size, image_size, 1], tf.float32)})\n",
    "\n",
    "    return tf.parse_single_example(serialized, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46f5c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plot(x, y, title):\n",
    "    \"\"\"Create a pyplot plot and save to buffer.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(x, y)\n",
    "    print(x.shape, y.shape)\n",
    "    plt.title(title)\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fd1cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.config import merge_configs_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "943f774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = merge_configs_recursive([{}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e06eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = get_path(cfg)\n",
    "train_dir = os.path.join(train_dir, str(cfg.vox_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da55d51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../Datasets/2Dpm/tf_records_new/03001627_train.tfrecords\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "split_name = 'train'\n",
    "dataset_file = os.path.join(\"../../../../Datasets/2Dpm/tf_records_new/\", f\"{cfg['synth_set']}_{split_name}.tfrecords\")\n",
    "print(dataset_file)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(dataset_file, compression_type=tf_record_compression(cfg))\n",
    "dataset = dataset.map(lambda rec: parse_tf_records(cfg, rec), num_parallel_calls=4) \\\n",
    "        .batch(cfg.batch_size) \\\n",
    "        .prefetch(buffer_size=100) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5e398fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "train_data = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27ace0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=88, shape=(1, 1), dtype=string, numpy=array([[b'7fe64a3a70f8f6b28cd4e3ad2fcaf039']], dtype=object)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3021ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.train.get_or_create_global_step()\n",
    "model = models.ModelPointCloud(cfg, global_step)\n",
    "inputs = model.preprocess(train_data, cfg.step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d453acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_to_keep = 120\n",
    "saver = tf.train.Saver(max_to_keep=max_to_keep)\n",
    "\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=False)\n",
    "session_config.gpu_options.allow_growth = cfg.gpu_allow_growth\n",
    "session_config.gpu_options.per_process_gpu_memory_fraction = cfg.per_process_gpu_memory_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75880e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 128, 128, 3)\n",
      "(4, 128, 128, 3)\n",
      "(4, 128, 128, 3)\n",
      "(4, 128, 128, 3)\n",
      "(4, 128, 128, 3)\n",
      "(4, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=session_config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        summary_writer = tf.summary.FileWriter(train_dir, flush_secs=10, graph=sess.graph)\n",
    "        count = 0\n",
    "        while count <= 5:\n",
    "            images = sess.run([inputs['images']])[0]\n",
    "            print(images.shape)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "306cb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 40, 40, 3) (4, 512)\n",
      "(1, 2048) (1, 40, 40, 12)\n",
      "WARNING:tensorflow:From /home/sirish/MTP_Thesis/2Dpm_EPCG/2D_projection_matching/2Dpm/main/../models/models.py:177: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "[<tf.Variable 'encoder/Conv/weights:0' shape=(5, 5, 3, 16) dtype=float32_ref>, <tf.Variable 'encoder/Conv/batch_normalization/gamma:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'encoder/Conv/batch_normalization/beta:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_1/weights:0' shape=(3, 3, 16, 32) dtype=float32_ref>, <tf.Variable 'encoder/Conv_1/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_1/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_2/weights:0' shape=(3, 3, 32, 32) dtype=float32_ref>, <tf.Variable 'encoder/Conv_2/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_2/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_3/weights:0' shape=(3, 3, 32, 64) dtype=float32_ref>, <tf.Variable 'encoder/Conv_3/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_3/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_4/weights:0' shape=(3, 3, 64, 64) dtype=float32_ref>, <tf.Variable 'encoder/Conv_4/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_4/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_5/weights:0' shape=(3, 3, 64, 128) dtype=float32_ref>, <tf.Variable 'encoder/Conv_5/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_5/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_6/weights:0' shape=(3, 3, 128, 128) dtype=float32_ref>, <tf.Variable 'encoder/Conv_6/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_6/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_7/weights:0' shape=(3, 3, 128, 256) dtype=float32_ref>, <tf.Variable 'encoder/Conv_7/batch_normalization/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_7/batch_normalization/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_8/weights:0' shape=(3, 3, 256, 256) dtype=float32_ref>, <tf.Variable 'encoder/Conv_8/batch_normalization/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'encoder/Conv_8/batch_normalization/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected/weights:0' shape=(4096, 1024) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected/batch_normalization/gamma:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected/batch_normalization/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_1/weights:0' shape=(1024, 1024) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_1/batch_normalization/gamma:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_1/batch_normalization/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_2/weights:0' shape=(1024, 1024) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_2/batch_normalization/gamma:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_2/batch_normalization/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_3/weights:0' shape=(1024, 512) dtype=float32_ref>, <tf.Variable 'encoder/fully_connected_3/biases:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected/weights:0' shape=(512, 1024) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected/batch_normalization/gamma:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected/batch_normalization/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_1/weights:0' shape=(1024, 2048) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_1/batch_normalization/gamma:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_1/batch_normalization/beta:0' shape=(2048,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_2/weights:0' shape=(2048, 4096) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_2/batch_normalization/gamma:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'decoder/fully_connected_2/batch_normalization/beta:0' shape=(4096,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose/weights:0' shape=(3, 3, 192, 256) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose/batch_normalization/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose/batch_normalization/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_1/weights:0' shape=(3, 3, 128, 192) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_2/weights:0' shape=(3, 3, 96, 128) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_2/batch_normalization/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_2/batch_normalization/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_3/weights:0' shape=(9, 9, 64, 96) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_3/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'decoder/Conv2d_transpose_3/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'decoder/pixelconv/weight:0' shape=(1, 1, 64, 4) dtype=float32_ref>, <tf.Variable 'decoder/pixelconv/bias:0' shape=(1, 40, 40, 4) dtype=float32_ref>]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'getvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-df6152fa095e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mloss_summary_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mpred_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mgt_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted_Image\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'getvalue'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "model_fn = model.get_model_fn(\n",
    "    is_training=True, reuse=False, run_projection=True)\n",
    "\n",
    "outputs = model_fn(inputs)\n",
    "\n",
    "# train_scopes\n",
    "train_scopes = ['encoder', 'decoder']\n",
    "\n",
    "task_loss, c_loss, k_loss, de_loss = model.get_loss(inputs, outputs)\n",
    "reg_loss = regularization_loss(train_scopes, cfg)\n",
    "loss = task_loss\n",
    "\n",
    "learning_rate = get_learning_rate(cfg, global_step)\n",
    "tf.summary.scalar(\"Learning_Rate\", learning_rate)\n",
    "# Merge all Summaries\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "var_list = get_trainable_variables(train_scopes)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss, global_step, var_list)\n",
    "\n",
    "# Epoch Loss summary\n",
    "tf.summary.scalar(\"Epoch_Loss\", per_epoch_loss)\n",
    "loss_summary_op = tf.summary.merge_all()\n",
    "\n",
    "pred_image = tf.image.decode_png(pred_buf.getvalue(), channels=4)\n",
    "gt_image = tf.image.decode_png(gt_buf.getvalue(), channels=4)\n",
    "tf.summary.image(\"Predicted_Image\", pred_image)\n",
    "tf.summary.image(\"Target_Image\", gt_image)\n",
    "im_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1ce5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1588d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ee156c75ef23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mglobal_step_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'session_config' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=session_config) as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "        summary_writer = tf.summary.FileWriter(train_dir, flush_secs=10, graph=sess.graph)\n",
    "        global_step_val = 0\n",
    "        epoch_loss = 0\n",
    "        t0 = time.perf_counter()\n",
    "        while global_step_val <= cfg['max_number_of_steps']:\n",
    "            _, loss_val, global_step_val, summary, result, gtpoints = sess.run([train_op, loss, global_step, summary_op, outputs, inputs['inpoints']])\n",
    "            print(summary)\n",
    "            summary_writer.add_summary(summary, global_step_val)\n",
    "            temp = result['all_points']\n",
    "            points3d = result['points3D']\n",
    "            assert temp[0].all() == temp[1].all()\n",
    "            assert temp[0].all() == points3d.all()\n",
    "            is_nan = np.isnan(loss_val)\n",
    "            assert(not np.any(is_nan))\n",
    "            epoch_loss += loss_val\n",
    "            \n",
    "            if global_step_val % 2 == 0 and global_step_val > 0:\n",
    "                pred = result['test_o'][0]\n",
    "                gt = gtpoints[0]\n",
    "                \n",
    "                predx = np.reshape(pred[:, 0], [pred.shape[0], 1])\n",
    "                predy = np.reshape(pred[:, 1], [pred.shape[0], 1])\n",
    "\n",
    "                gtx = np.reshape(gt[:, 0], [gt.shape[0], 1])\n",
    "                gty = np.reshape(gt[:, 1], [gt.shape[0], 1])\n",
    "\n",
    "                pred_buf = gen_plot(predx, predy, \"Predicted Data\")\n",
    "                gt_buf = gen_plot(gtx, gty, \"Target Image\")\n",
    "                \n",
    "                im_summary = sess.run(im_summary_op, feed_dict = {pred_buf: pred_buf, gt_buf: gt_buf})\n",
    "                print(im_summary)\n",
    "                summary_writer.add_summary(im_summary, global_step_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c349f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b5197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
